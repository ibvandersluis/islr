summary(gam.m3)
# Make predictions
preds=predict(gam.m2,newdata=Wage)
# We can use local regression as building blocks in a GAM with lo()
gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
plot.Gam(gam.lo,se=TRUE,col="green")
# We can also use lo() to create interactions before calling the gam() function
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
# We can also use lo() to create interactions before calling the gam() function
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
# We can also use lo() to create interactions before calling the gam() function
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
# To plot resulting 2D surface, install akima
install.packages('akima')
library(akima)
plot(gam.lo.i)
# To fit logistic regression GAM, use I() to make binary response variable with family=binomial
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")
# There are no high earners in the <HS category
table(education,I(wage>250))
# So we can remove this category from the GAM
gam.lr.s=gam(I(wage>250)âˆ¼year+s(age,df=5)+education,family=binomial,data=Wage,subset=(education!="1. < HS Grad"))
# So we can remove this category from the GAM
gam.lr.s=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,subset=(education!="1. < HS Grad"))
plot(gam.lr.s,se=T,col="green")
# Save history
savehistory()
# Quit
quit()
# Quit
q()
# Clear workspace
rm(list=ls())
install.packages('tree')
# If not already installed, install.packages('tree')
library(tree)
# Task: analyse Carseats data
# Recode Sales as binary variable
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"no","yes")
# Creat data frame to merge High with Carseats
Carseats=data.frame(Carseats,High)
# Fit classification tree to predict High using all variables except Sales
tree.carseats=tree(High~.-Sales,Carseats)
# Summarise tree
summary(tree.carseats)
# Plot the tree, with pretty=0 to include category names
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
# Split into test and train sets and predict results
set.seed(2)
train=sample(1:nrow(Carseats),200)
Carseats.test=Carseats[-train,]
High.test=High[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(104+50)/200
# Use pruning to attempt to improve model
# Determine optimal tree complexity with cv.tree()
set.seed(3)
# Determine optimal tree complexity with cv.tree()
# FUN=prune.misclass indicates we want classification error rate to guide cv and pruning
# As opposed to the cv.tree() default which is deviance
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
# Different values from book
attr(,"class")
# Different values from book
attr("class")
# Different values from book
attr(,"class")
# Different values from book
attr( ,"class")
# Plot error rate as function of size and k
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# Apply prune.misclass() to prune tree down to 8 nodes (book says 9 but my results differ)
prune.carseats=prune.misclass(tree.carseats,best=8)
plot(prune.carseats)
text(prune.carseats,pretty=0)
# Determine accuracy
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(89+62)/200
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.caarseats,best=15)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=15)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=10)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=11)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=12)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
library(MASS)
set.seed(1)
train=sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
# Determine if pruning helps
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
test(prune.boston,pretty=0)
text(prune.boston,pretty=0)
# Use unpruned tree to make predictions
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
# Apply bagging and random forests to Boston data
install.packages('randomForest')
# Apply bagging and random forests to Boston data
# If not already installed, install.packages('randomForest')
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag)
abline(0,1)
# That doesn't look right...
mean((yhat.bag-boston.test)^2)
# Change number of trees grown by randomForest() using ntree arg
bag.boston=randomForest(medv~.,data=Boston.subset=train,mtry=13,tree=25)
# Change number of trees grown by randomForest() using ntree arg
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,tree=25)
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
# Try with mtry=6
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf=predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
# View importance of variables
importance(rf.boston)
varImpPlot(rf.boston)
install.packages('gbm')
# If not already installed, install.packages('gbm')
library(gbm)
# Run gbm() with distribution="gaussian" because this is regression
# If for classification, use distribution="bernoulli"
# Specify number of trees with n.trees=5000
# Limit depth with interaction.depth=4
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
# Produce partial dependence plots, showing marginal effect of selected variables on response
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Produce partial dependence plots, showing marginal effect of selected variables on response
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Produce partial dependence plots, showing marginal effect of selected variables on response
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Use boosted model to predict medv on test set
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
# Perform boosting with different shrinkage parameter (lambda). Default = 0.001
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat,boost-boston.test)^2)
mean((yhat.boost-boston.test)^2)
# Save history
savehistory()
# Quit
quit()
# Quit
quit()
# Clear workspace
rm(list=ls())
install.packages('e1071')
library(e1071)
# Create support vector classifier for a given cost value
# Demonstrate with 2D example
set.seed(1)
x=matrix(rnorm(20*2),ncol=2)
y=c(rep(-1,10),rep(1,10))
x[y==1,]=x[y==1,]+1
plot(x,col=(3-y))
# Create data frame with respsonse coded as a factor
dat=data.frame(x=x,y=as.factor(y))
library(e1071)
svmfit=svm(y~.,data=dat,kernel="linear",cost=10,scale=FALSE)
# Plot the support vector
plot(svmfit,dat)
# Determine support vectors
svmfit$index
# Get information about the fit
summary(svmfit)
# Try with smaller cost
svmfit=svm(y~.,data=dat,kernel="linear",cost=0.1,scale=FALSE)
plot(svmfit,dat)
svmfit$index
# Use tune() to perform cross-validation
set.seed(1)
tune.out=tune(svm,y~.,data=dat,kernel="linear",ranges=list(cost=c(0.001,0.01,0.1,1, 5, 10, 100)))
summary(tune.out)
# Some variance in outputs
# Lowest cross-validation error rate is 0.5 (book says 0.1)
# Access the best model from tune() like this:
bestmod=tune.out$best.model
bestmod
summary(bestmod)
# Use predict() to categorise the label on a set of test obsrevations
xtest=matrix(rnorm(20*2),ncol=2)
ytest=sample(c(-1,1),20,rep=TRUE)
xtest[ytest==1,]=xtest[ytest==1,]+1
testdat=data.frame(x=xtest,y=as.factor(ytest))
table(predict=ypred,truth=testdat$y)
# Make predictions
ypred=predict(bestmod,testdat)
table(predict=ypred,truth=testdat$y)
# Try again with cost=0.01
svmfit=svm(y~.,data=dat,kernel="linear",cost=0.1,scale=FALSE)
ypred=predict(svmfit,testdat)
table(predict=ypred,truth=testdat$y)
# Make linearly separable
x[y==1,]=x[y==1,]+0.5
plot(x,col=(y+5)/2,pch=19)
# Fit model with high cost value so that no observations are misclassified
dat=data.frame(x=x,y=as.factory(y))
# Fit model with high cost value so that no observations are misclassified
dat=data.frame(x=x,y=as.factor(y))
svmfit=svm(y~.,data=dat,kernel="linear",cost=1e5)
summary(svmfit)
plot(svmfit,dat)
# Try smaller cost
svmfit=svm(y~.,data=dat,kernel="linear",cost=1)
summary(svmfit)
summary(svmfit)
plot(svmfit,dat)
# Other kenrel options: kernel="polynomial" and kernel="radial"
# Generate data with nonlinear class boundary
set.seed(1)
x=matrix(rnorm(200*2),ncol=2)
x[1:100,]=x[1:100,]+2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(x,col=y)
# Split into test/train groups
train=sample(200,100)
# Fit using svm() function
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1)
plot(svmfit,dat[train,])
summary(svmfit)
summary(svmfit)
# Inreasing cost will reduce training errors but at the price of irrecular boundary and maybe overfitting
svmfit=svm(y.,data=dat[train,],kernel="radial",gamma=1,cost=1e5)
# Inreasing cost will reduce training errors but at the price of irrecular boundary and maybe overfitting
svmfit=svm(y~.,data=dat[train,],kernel="radial",gamma=1,cost=1e5)
plot(svmfit,dat[train,])
set.seed(1)
tune.out(tune(svm,y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4))))
tune.out=tune(svm,y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4))))
tune.out=tune(svm,y~.,data=dat[train,],kernel="radial",ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
# My results give best cost as 100 and est gamma as 3
table(true=dat[-train,"y"],pred=predict(tune.out$best.model,newdata=dat[-train,]))
install.packages('ROCR')
library(ROCR)
rocplot=function(pred,truth,...){
predob=prediction(pred,truth)
perf=performance(predob,"tpr","fpr")
plot(perf,...)
}
svmfit.opt=svm(y~.,data=dat[train,],kernel="radial",gamma=2,cost=1,decision.values=T)
fitted=attributes(predict(svmfit.opt,dat[train,],decision.values=TRUE))$decision.values
par(mfrow=c(1,2))
rocplot(fitted,dat[train,"y"],main="Training Data")
# We can increase gamma to get a more flexible fit and further improve accuracy
svmfit.flex=svm(y~.,data=dat[train,],kernel="radial",gamma=50,cost=1,decision.values=T)
fitted=attributes(predict(svmfit.flex,dat[train,],decision.values=T))$decision.values
rocplot(fitted,dat[train,"y"],add=T,col="red")
# Compute ROC curves on test data
fitted=attributes(predict(svmfit.opt,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],main="Test Data")
fitted=attributes(predict(svmfit.flex,dat[-train,],decision.values=T))$decision.values
rocplot(fitted,dat[-train,"y"],add=T,col="red")
# Generate a third class of observations
set.seed(1)
x=rbind(x,matrix(rnorm(50*2),ncol=2))
y=c(y,rep(0,50))
x[y==0,2]=x=[y==0,2]+2
x[y==0,2]=x[y==0,2]+2
dat=data.frame(x=x,y=as.factor(y))
par(mfrow=c(1,1))
plot(x,col=y+1)
# Fit SVM to data
svmfit=svm(y~.,data=dat,kernel="radial",cost=10,gamma=1)
plot(svm,dat)
plot(svmfit,dat)
# Gene expression measurements are available for each tissue sample
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
dim(Khan$ytrain)
length(Khan$ytrain)
length(Khan$ytest)
# Training and test sets hold 63 and 20 observations respectively
table(Khan$ytrain)
table(Khan$ytest)
# Relatively few observations compared with features --> use linear kernel
dat=data.frame(x=Khan$xtrain,y=as.factor(Khan$ytrain))
out=svm(y~.,data=dat,kernel="linear",cost=10)
summary(out)
table(out$fitted,dat$y)
# Try on test data
dat.te=data.frame(x=Khan$xtest,y=as.factor(Khan$ytest))
pred.te=predict(out,newdata=dat.te)
table(pred.te,dat.te$y)
# Save history
savehistory()
# Quit
q()
# Clear workspace
rm(list=ls())
# Objective: perform PCA on USArrests dataset, part of the base R package
states=row.names(USArrests)
states
# Inspect the variables
names(USArrests)
# Examine data
apply(USArrests,2,mean)
# Examine variances
apply(USArrests,2,var)
# Perform principal components analysis
pr.out=prcomp(USArrests,scale=TRUE)
names(pr.out)
# center and scale are the means and standard deviations of variables used for scaling
pr.out$center
pr.out$scale
# rotation matrix gives principal component loading vectors
pr.out$rotation
# NOTE: in general there are min(n-1,p) informative principal components in a set of n observations and p variables
# The x matrix has the principal component score vector
dim(pr.out$x)
# Plot first two principal components
biplot(pr.out,scale=0)
# This is a mirror image from Figure 10.1 in the book
# This is because principal components are unique up to a sign change
# We can replicate Figure 10.1 by doing:
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out,scale=0)
# Get standard deviation of each principal component
pr.out$sdev
# Get variance explained by squaring standard deviations
pr.var=pr.out$sdev^2
pr.var
# Compute proportion of variance explained by each principal component
# Divide variance explained (by each principal component) by total variance explained (by all principal components)
pve=pv.var/sum(pr.var)
# Compute proportion of variance explained by each principal component
# Divide variance explained (by each principal component) by total variance explained (by all principal components)
pve=pr.var/sum(pr.var)
pve
# Plot PVE per component
plot(pve,xlab="Principal Component",ylab="Proportion of Variance Explained",ylim=c(0,1),type="b")
# Plot cumulative PVE
plot(cumsum(pve),xlab="Principal Component",ylab="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
# cumsum() computes cumulative sum of elements of a numeric vector. Example:
a=c(1,2,8,-3)
cumsum(a)
# Simple example, in which there truly are two distinct clusters in the data
set.seed(2)
x=matrix(rnorm(50*2),ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
# Perform K-means clustering with K=2
km.out=kmeans(x,2,nstart=20)
# Get cluster assignments
km.out$cluster
# Plot data with colour-coded observations
plot(x,col=(km.out$cluster+1),main="K-Means Clustering Results with K=2",xlab="",ylab="",pch=20,cex=2)
# Try with K=3
set.seed(4)
km.out=kmeans(x,3,nstart=20)
km.out
# Try with K=3 and nstart=1 vs nstart=20
set.seed(3)
km.out=kmeans(x,3,nstart=1)
km.out$tot.withinss
km.out=kmeans(x,3,nstart=20)
km.out$tot.withinss
# Use hclust() to use hierarchical clustering. dist() computes a 50x50 inter-observation Euclidian distance matrix
hc.complete=hclust(dist(x),method="complete")
# Perform hierarchical clustering with average or single linkage
hc.average=hclust(dist(x),method="average")
hc.single=hclust(dist(x),method="single")
# Plot the dendrograms
par(mfrow=c(1,3))
plot(hc.complete,main="Complete Linkage",xlab="",sub="",cex=.9)
plot(hc.average,main="Average Linkage",xlab="",sub="",cex=.9)
plot(hc.single,main="Single Linkage",xlab="",sub="",cex=.9)
# Determine cluster labels for each observation associated with a given cut of the dendrogram
cutree(hc.complete,2)
cutree(hc.average,2)
cutree(hc.single,2)
# Note that single linkage as only one observation in its own cluster at clusters=2
# Try single linkage with 4 clusters (two singletons still remain)
cutree(hc.single,4)
# Use scale() to scale variables before performing clustering
xsc=scale(x)
plot(hclust(dist(xsc),method="complete"),main="Hierarchical Clustering with Scaled Features")
par(mfrow=c(1,1))
plot(hclust(dist(xsc),method="complete"),main="Hierarchical Clustering with Scaled Features")
# Compute correlation-based distance with as.dist()
# This only makes sense for data with > 3 features
# Cluster a 3-dimensional dataset
x=matrix(rnorm(30*3),ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd,method="complete"),main="Complete Linkage with Correlation-Based Distance",xlab="",sub="")
# Use PCA and hierarchical clustering on NCI60 cancer cell line microarray data
# 6,830 gene expression measurements on 64 cancer cell lines
library(ISLR)
nci.labs=NCI60$labs
nci.data=NCI60$data
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
# Perform PCA after scaling data
pr.out=prcomp(nci.data,scale=TRUE)
# Create a function that assigns color to each element of a numeric vector
Cols=function(vec){
cols=rainbow(length(unique(vec)))
return(cols[as.numeric(as.factor(vec))])
}
# Plot first few principal component score vectors
par(mfrow=c(1,2))
plot(pr.out$x[,1:2],col=Cols(nci.labs),pch=19,xlab="Z1",ylab="Z2")
plot(pr.out$x[,c(1,3)],col=Cols(nci.labs),pch=19,xlab="Z1",ylab="Z3")
# Get summary of proportion of variance explained
summary(pr.out)
# Plot variance explained
plot(pr.out)
# Plot PVE of each principal component
pve=100*pr.out$sdev^2/sum(pr.out$sdev^2)
par(mfrow=c(1,2))
plot(pve,type="o",ylab="PVE",xlab="Principal Component",col="blue")
# Plot cumulative PVE
plot(cumsum(pve),type="o",ylab="Cumulative PVE",xlab="Principal Component",col="brown3")
# Standardise variables to have mean 0 and sd 1
sd.data=scale(nci.data)
# Perform hierarchical clustering using complete, single, and average linkage
par(mfrow=c(1,3))
data.dist=dist(sd.data)
plot(hclust(data.dist),labels=nci.labs,main="Complete Linkage",xlab="",sub="",ylab="")
plot(hclust(data.dist,method="average"),labels=nci.labs,main="Average Linkage",xlab="",sub="",ylab="")
plot(hclust(data.dist,method="single"),labels=nci.labs,main="Single Linkage",xlab="",sub="",ylab="")
# Cut the dendrogram at a height that will yield 4 clusters
hc.out=hclust(dist(sd.data))
hc.clusters=cutree(hc.out,4)
table(hc.clusters,nci.labs)
# Plot this cut
par(mfrow=c(1,1))
plot(hc.out,labels=nci.labs)
abline(h=139,col="red")
# Print output of hclust
hc.out
# Try k-means clustering with K=4
set.seed(2)
km.out=kmeans(sd.data,4,nstart=20)
km.clusters=km.out$cluster
table(km.clusters,hc.clusters)
# Perform hierarchical clustering on first few principal component score vectors
hc.out=hclust(dist(pr.out$x[,1:5]))
plot(hc.out,labels=nci.labs,main="Hier. Clust. on First Five Score Vectors")
table(cutree(hc.out,4),nci.labs)
# Save history
savehistory()
# Quit
q()
