install.packages('ISLR')
library(ISLR)
install.packages('MASS')
library(MASS)
x <- x(1,3,2,5)
# Clear variables
rm(list=ls())
x <- c(1,3,2,5)
# Print x
x
# Assign values to x and y using =
x = c(1,6,2)
y = c(1,4,3)
# Get lengths of x and y
length(x)
length(y)
# Add x and y
x+y
ls()
# Clear workspace
rm(list=ls())
# Assign matrix to x
x = matrix(data=c(1,2,3,4), nrow=2, ncol=2)
# Print x
x
# Remake matrix with byrow=TRUE
matrix(c(1,2,3,4),2,2,byrow=TRUE)
# Math on matrix x (performs operation on each object)
sqrt(x)
x^2
# Assign x 50 numbers with normal distribution
x=rnorm(50)
# Assign y similar numbers but changes distribution settings
y=x+rnorm(50,mean=50,sd=.1)
# Calculatre correlation of x and y
cor(x,y)
# Set the seed to 1303
set.seed(1303)
rnorm(50)
# Set the seed to 3
set.seed(3)
y=rnorm(100)
mean(y)
var(y)
sqrt(var(y))
sd(y)
# Assign normal distributions to x and y
x=rnorm(100)
y=rnorm(100)
# Plot x and y
plot(x,y)
# Plot x and y with labels
plot(x,y,xlab="this is the x-axis",ylab="this is the y-axis",main="Plot of X vs Y")
# Save as .pdf!
# Create empty .pdf file
pdf("Figure.pdf")
# Make a plot for the .pdf
plot(x,y,col="green")
# Tell R we're done making the .pdf
dev.off()
getwd()
setwd(~/code/name_UI <- function(id) {
ns <- NS(id)
tagList(
)
}
name <- function(input, output, session) {
})
setwd(~/code/src/github/ibvandersluis/islr)
setwd(/home/isaac/code/src/github/ibvandersluis/islr)
setwd("/home/isaac/code/src/github/ibvandersluis/islr")
setwd("~/code/src/github/ibvandersluis/islr")
setwd("~/code/src/github.com/ibvandersluis/islr")
getwd()
# Make sequence 1-10 (inclusive) and assign to x
x=seq(1,10)
# Print x
x
# Alternative method
x=1:10
x
# Assign x a sequence of 50 evely spaced numbers from -pi to pi
x=seq(-pi,pi,length=50)
x
# Make a contour (topo map) plot. Arguments: vectors x and y, and a matrix for z
y=x
f=outer(x,y,function(x,y)cos(y)/(1+x^2))
contour(x,y,f)
contour(x,y,f,nlevels=45,add=T)
fa=(f-t(f))/2
contour(x,y,fa,nlevels=15)
# Make an image (colour-coded plot)
image(x,y,fa)
persp(x,y,fa)
persp(x,y,fa,theta=30)
persp(x,y,fa,theta=30,phi=20)
persp(x,y,fa,theta=30,phi=70)
persp(x,y,fa,theta=30,phi=40)
# Store data in a matrix in A
A=matrix(1:16,4,4)
# Print A
A
# Select cell from row 2, column 3
A[2,3]
# Select portions of matrix
A[c(1,3), c(2,4)]
A[1:3,2:4]
A[1:2,]
A[,1:2]
A
# Use - sign to exclude that section
# Exclude rows 1 and 3
A[-c(1,3),]
# Exclude rows 1 and 3, and cols 1, 3 and 4
A[-c(1,3),-c(1,3,4)]
# Print dimensions of A
dim(A)
# Load Auto.data into R
Auto=read.table("Auto.data")
# View data in spreadsheet like window
fix(Auto)
# Load data, correctly sorting out header ('header=T') and missing values ('na.strings="?"')
Auto=read.table("Auto.data",header=T,na.strings="?")
# Reexamine datat
fix(Auto)
# Load from CSV
Auto=read.csv("Auto.csv",header=T,na.strings="?")
# Load from CSV
Auto=read.csv("Auto.csv",header=T,na.strings="?")
fix(Auto)
dim(Auto)
Auto[1:4,]
# Omit rows with absent values
Auto=na.omit(Auto)
dim(Auto)
# View variable names
names(Auto)
# Tell R to plot cylinders, mpg (will produce error)
plot(cylinders, mpg)
# Till R which data set to look in for the variable name
plot(Auto$cylinders, Auto$mpg)
# Alternatively, attach R to the dataset
attach(Auto)
plot(cylinders, mpg)
# Convert cylinders from quantitative to qualitative
cylinders=as.factor(cylinders)
plot(cylinders, mpg)
plot(cylinders, mpg, col="red")
plot(cylinders, mpg, col="red", varwidth=T)
plot(cylinders, mpg, col="red", varwidth=T)
plot(cylinders, mpg, col="red", varwidth=T, horizontal=T)
plot(cylinders, mpg, col="red", varwidth=T, xlab="cylinders", ylab="MPG")
# Plot a histogram
hist(mpg)
hist(mpg,col=2)
hist(mpg,col=2,breaks=15)
# Create a scatterplot matrix with pairs()
pairs(Auto)
pairs(~ mpg + displacement + horsepower + weight + acceleration, Auto)
# identify() allows us to interact with a plot to get values on specific data points
# 3 args: x-axis, y-axis, the variable to be examined
plot(horsepower,mpg)
identify(horsepower,mpg,name)
summary(Auto)
# Or just one variables
summary(mpg)
# Save commands from session
savehistory()
# Quit
q()
# Clear workspace
rm(list=ls())
# Libraries are groups of functions and datasets not included in base R distribution
# Load libraries MASS and ISLR
library(MASS)
library(ISLR)
# Goal: predict medv (median house value)
# Inspect Boston dataset
fix(Boston)
names(Boston)
?Boston
# Simple linear regression with medv as response and lstat as predictor
# Form: lm(y~x,data). y is response, x is predictor, data is dataset that stores them
# Returns error, need to specify dataset:
lm.fit=lm(medv~lstat)
# Provide data reference
lm.fit=lm(medv~lstat,data=Boston)
# Alternative method
attach(Boston)
lm.fit=lm(medv~lstat)
# Get info about lm.fit
lm.fit
summary(lm.fit)
names(lm.fit)
coef(lm.fit)
# Get confidence interval for coefficient estimates
confint(lm.fit)
# predict() confidence intervals and prediction intervals of medv for given value lstat
predict(lm.fit,data.frame(lstat=c(5,10,15)),interval="confidence")
predict(lm.fit,data.frame(lstat=c(5,10,15)),interval="prediction")
# Plot medv and lstat with least squares regression
plot(lstat,medv)
# abline() can draw any line abline(a,b) with intercept a and slope b
abline(lm.fit)
# abline() with different options. lwd=3 increases width by factor of 3
abline(lm.fit,lwd=3)
abline(lm.fit,lwd=3,col="red")
plot(lstat,medv,col="red")
plot(lstat,medv,pch=20)
plot(lstat,medv,pch="+")
plot(1:20,1:20,pch=1:20)
# Some diagnostic plots
# Split display into 2x2 grid
par(mfrow=c(2,2))
# Plot
plot(lm.fit)
# Use residuals() and rstudent() to return residuals and studentised residuals
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
# Compute leverate statistics
plot(hatvalues(lm.fit))
# Return index of largest element in vector
which.max(hatvalues(lm.fit))
# lm() for multiple regression syntax is lm(y~x1+x2+x3) for 3 predictors
lm.fit=lm(medv~lstat+age,data=Boston)
# Get coefficients
summary(lm.fit)
# Use . shorthand to denote all variables in order
lm.fit=lm(medv~.,data=Boston)
# Get coefficients
summary(lm.fit)
# Just get R squared
summary(lm.fit)$r.sq
# If not installed: install.packages("car")
# Ran into a problem installing the car package due to unsuccessful installation of lme4. This was resolved.
# See https://community.rstudio.com/t/issue-installing-car-package/58319 for my response detailing what I did.
# Load car package
library(car)
# Compute variance inflation factors
vif(lm.fit)
# Perform regression using all but one variable (age)
lm.fit1=lm(medv~.-age,data=Boston)
summary(lm.fit1)
# Alternative method
lm.fit1=update(lm.fit, ~.-age)
summary(lm.fit1)
# Syntax of terms
# a:b includes an interaction term between a and b
# a*b includes a, b, and the interaction term aXb
# In otherwords a*b = a+b+a:b
summary(lm(medv~lstat*age,data=Boston))
# lstat^2 must be stored in the function I()
lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
# Quantify benefit of quadratic fit over linear fit
lm.fit=lm(medv~lstat)
anova(lm.fit,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
# Use poly() function for high-order polynomials within lm()
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
# Example of logarithmic transformation
summary(lm(medv~log(rm),data=Boston))
# Goal: attempt to predict Sales in 400 locations
fix(Carseats)
names(Carseats)
# R generates dummy variables for qualitative variables automatically
# Multiple regression model with some interaction terms
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
# contrasts() returns coding that R uses for dummy variables
attach(Carseats)
contrasts(ShelveLoc)
?contrasts
# Error, no such object
LoadLibraries
# Error: no such function
LoadLibraries()
# Create function LoadLibraries()
# { signals R to allows carriage returns for more commands, } signals are that we are done entering commands
LoadLibraries=function(){
library(ISLR)
library(MASS)
print("The libraries have been loaded.")
}
# Typing the function name returns the contents of the function
LoadLibraries
# Calling the function executes the instructions in the function
LoadLibraries()
# Save history
savehistory()
# Quit
q()
# Quit
q()
# Clear workspace
rm(list=ls())
# Load ISLR library
library(ISLR)
# Inspect Smarket
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
# cor() gives a matrix containing all pairwise correlations from predictors in the dataset
# Gives error, Direction variable is qualitative
cor(Smarket)
# Exclude Direction
cor(Smarket[,-9])
# Shows little correlation between today's returns and yesterday's returns
# Notable correlation: Year-Volume
# Plot Volume
attach(Smarket)
plot(Volume)
# glm() fits generalised linear models, which includes logistic regression
# Use arg family=binomial to indicate R should run logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial)
summary(glm.fit)
# Get coefficients for this model
coef(glm.fit)
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
# Use predict() to determine probability that market will go up given the predictors
# type="response" tells R to output probabilities of P(Y=1|X) instead of other info
# If predict() receives no dataset, probabilities are computed using log regression training data
glm.probs=predict(glm.fit,type="response")
glm.probs[1:10]
# Show dummy variable
contrasts(Direction)
# Create a vector of class predictions based on whether probability of market increase >= 0.5
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
# Use table() to produce confusion matrix for determining correct/incorrect classifications
table(glm.pred,Direction)
# From output, calculate how many were correct
# i.e. in how many cases did the market go up when we predicted up, or down when we predicted down
(507+145)/1250
mean(glm.pred==Direction)
# Create vector corresponding to observations from 2001-2004
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
View(Smarket.2005)
Direction.2005
# Fit logistic regression model using observations < 2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial,subset=train)
# Obtain predictions of stock market for 2005 observations
glm.probs=predict(glm.fit,Smarket.2005,type="response")
# Compare predictions to observations for 2005
glm.pred=rep("Down",252)
# If prob > 0.5, set to 'Up'
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
(77+44)/252
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)
# Let's try removing predictors that are less helpful: Lag3, Lag4, Lag5
# Refit logistic regression with only Lag1 and Lag2
glm.fit=glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
# 56% accuracy
# Accuracy on days when a market increase is predicted
106/(106+76)
# Predict returns associated with values Lag1=1.2 & Lag2=1.1, and Lag1=1.5 & Lag2=-0.8
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
# Load MASS
library(MASS)
# Use lda() function. Syntax same as for lm(), and glm() except for absence of family option
# Fit model using observations before 2005
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket,subset=train)
lda.fit
plot(lda.fit)
lda.pred=predict(lda.fit,Smarket.2005)
names(lda.pred)
lda.class=lda.pred$class
table(lda.class,Direction.2005)
mean(lda.class==Direction.2005)
lda.class
# Apply 50% threshold to posterior probabilites to recreate predictions form lda.pred$class
sum(lda.pred$posterior[,1]>=.5)
sum(lda.pred$posterior[,1]<.5)
# Note that posterior probability output is the probability of market decrease
lda.pred$posterior[1:20,1]
lda.class[1:20]
# Change the threshold to 90%
sum(lda.pred$posterior[,1]>.9)
# Fit QDA model to Smarket. Syntax of qda() identical to lda()
qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
table(qda.class,Direction.2005)
qda.class=predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
mean(qda.class==Direction.2005)
# Load class library for knn() function
library(class)
# Unlike the previous functions where we get predictions by first fitting the model,
# knn() forms predictions with a single command
# knn() inputs:
# - A matrix containing predictors associated with training data (train.X)
# - A matrix containing predictors associated with testing data (text.X)
# - A vector containing class labels for training observations (train.Direction)
# - A value for K, the number of neighbours used by the classifier
# Use column bind cbind() to bind Lag1 and Lag2
train.X=cbind(Lag1,Lag2)[train,]
test.X=cbind(Lag1,Lag2)[!train,]
train.Direction=Direction[train]
# Seed random
set.seed(1)
knn.pred=knn(train.X,test.X,train.Direction,k=1)
# Get accuracy
table(knn.pred,Direction.2005)
(83+43)/252
# 50% accuracy, not great
# Try again with different value for K
knn.pred=knn(train.X,text.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
# 50% accuracy, not great
# Try again with different value for K
knn.pred=knn(train.X,test.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
# Apply KNN to Caravan dataset
# 85 predictors for 5,822 people
# Response variable: Purchase. Does the individual purchase an insurance policy?
# 6% purchased insurance
dim(Caravan)
attach(Caravan)
summary(Purchas)
summary(Purchase)
348/5822
# 5.97% Purchased insurance
# Use scale() function to scale the data
# Exclude col 86 (Purchase)
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
# Split observations into test set (1,000) and a training set (remainder)
# All columns now have standard deviation of 1 and a mean of zero
# Split observations into test set (1,000) and a training set (remainder)
test=1:1000
train.X=standardized.X[-test,]
test.X=standardized.X[test,]
train.Y=Purchase[-test,]
train.Y=Purchase[-test]
test.Y=Purchase[test]
set.seed(1)
knn.pred=knn(train.X,test.X,train.Y,k=1)
mean(test.Y!=knn.pred)
mean(test.Y!="No")
# But positive response rate is about half that at 5.9%
table(knn.pred,test.Y)
9/(68+9)
# K=1 -- 11.7% of those predicted to buy actually did
# Try for K=3
knn.pred=knn(train.X,test.X,train.Y,k=3)
table(knn.pred,test.Y)
5/26
# K=3 -- 19.2%
# Try for K=5
knn.pred=knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
4/15
# Fit logistic regression model to data
glm.fit=glm(Purchase~.,data=Caravan,family=binomial,subset=-test)
glm.probs=predict(glm.fit,Caravan[test,],type="response")
glm.pred=rep("No",1000)
glm.pred[glm.probs>.5]="Yes"
table(glm.pred,test.Y)
# Only 7 people are predicted to buy at a 0.5 threshold, and all 7 predictions were wrong
# Change threshold to 0.25
glm.pred=rep("No",1000)
glm.pred[glm.probs>.25]="Yes"
table(glm.pred,test.Y)
11/(22+11)
# Save history
savehistory()
# Save history
savehistory()
# Quit
q()
