lm.fit2=lm(medv~lstat+I(lstat^2))
summary(lm.fit2)
# Quantify benefit of quadratic fit over linear fit
lm.fit=lm(medv~lstat)
anova(lm.fit,lm.fit2)
par(mfrow=c(2,2))
plot(lm.fit2)
# Use poly() function for high-order polynomials within lm()
lm.fit5=lm(medv~poly(lstat,5))
summary(lm.fit5)
# Example of logarithmic transformation
summary(lm(medv~log(rm),data=Boston))
# Goal: attempt to predict Sales in 400 locations
fix(Carseats)
names(Carseats)
# R generates dummy variables for qualitative variables automatically
# Multiple regression model with some interaction terms
lm.fit=lm(Sales~.+Income:Advertising+Price:Age,data=Carseats)
summary(lm.fit)
# contrasts() returns coding that R uses for dummy variables
attach(Carseats)
contrasts(ShelveLoc)
?contrasts
# Error, no such object
LoadLibraries
# Error: no such function
LoadLibraries()
# Create function LoadLibraries()
# { signals R to allows carriage returns for more commands, } signals are that we are done entering commands
LoadLibraries=function(){
library(ISLR)
library(MASS)
print("The libraries have been loaded.")
}
# Typing the function name returns the contents of the function
LoadLibraries
# Calling the function executes the instructions in the function
LoadLibraries()
# Save history
savehistory()
# Quit
q()
# Quit
q()
# Clear workspace
rm(list=ls())
# Load ISLR library
library(ISLR)
# Inspect Smarket
names(Smarket)
dim(Smarket)
summary(Smarket)
pairs(Smarket)
# cor() gives a matrix containing all pairwise correlations from predictors in the dataset
# Gives error, Direction variable is qualitative
cor(Smarket)
# Exclude Direction
cor(Smarket[,-9])
# Shows little correlation between today's returns and yesterday's returns
# Notable correlation: Year-Volume
# Plot Volume
attach(Smarket)
plot(Volume)
# glm() fits generalised linear models, which includes logistic regression
# Use arg family=binomial to indicate R should run logistic regression
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial)
summary(glm.fit)
# Get coefficients for this model
coef(glm.fit)
summary(glm.fit)$coef
summary(glm.fit)$coef[,4]
# Use predict() to determine probability that market will go up given the predictors
# type="response" tells R to output probabilities of P(Y=1|X) instead of other info
# If predict() receives no dataset, probabilities are computed using log regression training data
glm.probs=predict(glm.fit,type="response")
glm.probs[1:10]
# Show dummy variable
contrasts(Direction)
# Create a vector of class predictions based on whether probability of market increase >= 0.5
glm.pred=rep("Down",1250)
glm.pred[glm.probs>.5]="Up"
# Use table() to produce confusion matrix for determining correct/incorrect classifications
table(glm.pred,Direction)
# From output, calculate how many were correct
# i.e. in how many cases did the market go up when we predicted up, or down when we predicted down
(507+145)/1250
mean(glm.pred==Direction)
# Create vector corresponding to observations from 2001-2004
train=(Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
Direction.2005=Direction[!train]
View(Smarket.2005)
Direction.2005
# Fit logistic regression model using observations < 2005
glm.fit=glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,data=Smarket,family=binomial,subset=train)
# Obtain predictions of stock market for 2005 observations
glm.probs=predict(glm.fit,Smarket.2005,type="response")
# Compare predictions to observations for 2005
glm.pred=rep("Down",252)
# If prob > 0.5, set to 'Up'
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
(77+44)/252
mean(glm.pred==Direction.2005)
mean(glm.pred!=Direction.2005)
# Let's try removing predictors that are less helpful: Lag3, Lag4, Lag5
# Refit logistic regression with only Lag1 and Lag2
glm.fit=glm(Direction~Lag1+Lag2,data=Smarket,family=binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
glm.pred=rep("Down",252)
glm.pred[glm.probs>.5]="Up"
table(glm.pred,Direction.2005)
mean(glm.pred==Direction.2005)
# 56% accuracy
# Accuracy on days when a market increase is predicted
106/(106+76)
# Predict returns associated with values Lag1=1.2 & Lag2=1.1, and Lag1=1.5 & Lag2=-0.8
predict(glm.fit,newdata=data.frame(Lag1=c(1.2,1.5),Lag2=c(1.1,-0.8)),type="response")
# Load MASS
library(MASS)
# Use lda() function. Syntax same as for lm(), and glm() except for absence of family option
# Fit model using observations before 2005
lda.fit=lda(Direction~Lag1+Lag2,data=Smarket,subset=train)
lda.fit
plot(lda.fit)
lda.pred=predict(lda.fit,Smarket.2005)
names(lda.pred)
lda.class=lda.pred$class
table(lda.class,Direction.2005)
mean(lda.class==Direction.2005)
lda.class
# Apply 50% threshold to posterior probabilites to recreate predictions form lda.pred$class
sum(lda.pred$posterior[,1]>=.5)
sum(lda.pred$posterior[,1]<.5)
# Note that posterior probability output is the probability of market decrease
lda.pred$posterior[1:20,1]
lda.class[1:20]
# Change the threshold to 90%
sum(lda.pred$posterior[,1]>.9)
# Fit QDA model to Smarket. Syntax of qda() identical to lda()
qda.fit=qda(Direction~Lag1+Lag2,data=Smarket,subset=train)
qda.fit
table(qda.class,Direction.2005)
qda.class=predict(qda.fit,Smarket.2005)$class
table(qda.class,Direction.2005)
mean(qda.class==Direction.2005)
# Load class library for knn() function
library(class)
# Unlike the previous functions where we get predictions by first fitting the model,
# knn() forms predictions with a single command
# knn() inputs:
# - A matrix containing predictors associated with training data (train.X)
# - A matrix containing predictors associated with testing data (text.X)
# - A vector containing class labels for training observations (train.Direction)
# - A value for K, the number of neighbours used by the classifier
# Use column bind cbind() to bind Lag1 and Lag2
train.X=cbind(Lag1,Lag2)[train,]
test.X=cbind(Lag1,Lag2)[!train,]
train.Direction=Direction[train]
# Seed random
set.seed(1)
knn.pred=knn(train.X,test.X,train.Direction,k=1)
# Get accuracy
table(knn.pred,Direction.2005)
(83+43)/252
# 50% accuracy, not great
# Try again with different value for K
knn.pred=knn(train.X,text.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
# 50% accuracy, not great
# Try again with different value for K
knn.pred=knn(train.X,test.X,train.Direction,k=3)
table(knn.pred,Direction.2005)
mean(knn.pred==Direction.2005)
# Apply KNN to Caravan dataset
# 85 predictors for 5,822 people
# Response variable: Purchase. Does the individual purchase an insurance policy?
# 6% purchased insurance
dim(Caravan)
attach(Caravan)
summary(Purchas)
summary(Purchase)
348/5822
# 5.97% Purchased insurance
# Use scale() function to scale the data
# Exclude col 86 (Purchase)
standardized.X=scale(Caravan[,-86])
var(Caravan[,1])
var(Caravan[,2])
var(standardized.X[,1])
var(standardized.X[,2])
# Split observations into test set (1,000) and a training set (remainder)
# All columns now have standard deviation of 1 and a mean of zero
# Split observations into test set (1,000) and a training set (remainder)
test=1:1000
train.X=standardized.X[-test,]
test.X=standardized.X[test,]
train.Y=Purchase[-test,]
train.Y=Purchase[-test]
test.Y=Purchase[test]
set.seed(1)
knn.pred=knn(train.X,test.X,train.Y,k=1)
mean(test.Y!=knn.pred)
mean(test.Y!="No")
# But positive response rate is about half that at 5.9%
table(knn.pred,test.Y)
9/(68+9)
# K=1 -- 11.7% of those predicted to buy actually did
# Try for K=3
knn.pred=knn(train.X,test.X,train.Y,k=3)
table(knn.pred,test.Y)
5/26
# K=3 -- 19.2%
# Try for K=5
knn.pred=knn(train.X,test.X,train.Y,k=5)
table(knn.pred,test.Y)
4/15
# Fit logistic regression model to data
glm.fit=glm(Purchase~.,data=Caravan,family=binomial,subset=-test)
glm.probs=predict(glm.fit,Caravan[test,],type="response")
glm.pred=rep("No",1000)
glm.pred[glm.probs>.5]="Yes"
table(glm.pred,test.Y)
# Only 7 people are predicted to buy at a 0.5 threshold, and all 7 predictions were wrong
# Change threshold to 0.25
glm.pred=rep("No",1000)
glm.pred[glm.probs>.25]="Yes"
table(glm.pred,test.Y)
11/(22+11)
# Save history
savehistory()
# Save history
savehistory()
# Quit
q()
# Clear workspace
rm(list=ls())
savehistory()
q()
# Load the ISLR library
library(ISLR)
set.seed(1)
# Split observations into two halves
train=sample(392,196)
# Use subset option in lm() to fit a linear regression
lm.fit=lm(mpg~horesepower,data=Auto,subset=train)
# Use subset option in lm() to fit a linear regression
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
# Predict response for 392 observations
# Calculate MSE of the 196 observations in validation set
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# Use poly() to estimate test error for polynomial and cubic regressions
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg=predict(lm.fit2,Auto))[-train]^2)
# Set seed for reproducible pseudorandom results
set.seed(1)
mean((mpg=predict(lm.fit2,Auto))[-train]^2)
mean((mpg=predict(lm.fit2,Auto))[-train]^2)
mean((mpg=predict(lm.fit2,Auto))[-train]^2)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
# Clear workspace
rm(list=ls())
# Load the ISLR library
library(ISLR)
# Set seed for reproducible pseudorandom results
set.seed(1)
# Split observations into two halves
train=sample(392,196)
# Use subset option in lm() to fit a linear regression with observations corresponding to training data
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
# Predict response for 392 observations
# Calculate MSE of the 196 observations in validation set
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# Use poly() to estimate test error for polynomial and cubic regressions
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
# Clear workspace
rm(list=ls())
# Load the ISLR library
library(ISLR)
# Set seed for reproducible pseudorandom results
set.seed(1)
# Split observations into two halves
train=sample(392,196)
# Use subset option in lm() to fit a linear regression with observations corresponding to training data
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
# Predict response for 392 observations
# Calculate MSE of the 196 observations in validation set
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# Use poly() to estimate test error for polynomial and cubic regressions
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
# Clear workspace
rm(list=ls())
# Load the ISLR library
library(ISLR)
# Set seed for reproducible pseudorandom results
set.seed(1)
# Split observations into two halves
train=sample(392,196)
# Use subset option in lm() to fit a linear regression with observations corresponding to training data
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
# Predict response for 392 observations
# Calculate MSE of the 196 observations in validation set
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# Use poly() to estimate test error for polynomial and cubic regressions
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(2)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
# Using glm() function without passing the family argument, it performs linear regression
glm.fit=glm(mpg~horsepower,data=Auto)
coef(glm.fit)
lm.fit=lm(mpg~horsepower,data=Auto)
coef(lm.fit)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
# Use for() to automate the program
cv.error=rep(0,5)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
for(i in 1:5){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
alpha.fn=function(data.index){
X=data$X[index]
Y=data$Y[index]
return((var(Y)-cov(X,Y)/var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))
boot(Portfolio,alpha.fn,R=1000)
alpha.fn=function(data.index){
X=data$X[index]
Y=data$Y[index]
return((var(Y)-cov(X,Y)/var(X)+var(Y)-2*cov(X,Y)))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))
alpha.fn=function (data ,index){
X=data$X [index]
Y=data$Y [index]
return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))
boot(Portfolio,alpha.fn,R=1000)
return(coef(lm(mpg~horsepower,data=data,subset=index))
boot.fn=function(data,index)
boot.fn=function(data,index)
return(coef(lm(mpg~horsepower,data=data,subset=index))
boot.fn(Auto,1:329)
boot.fn=function(data,index)
return(coef(lm(mpg~horsepower,data=data,subset=index))
set.seed(1)
set.seed(1)
boot.fn(Auto,sample(392,392,replace=T))
boot.fn=function (data,index)
return(coef(lm(mpg~horsepower,data=data,subset=index))
set.seed(1)
boot.fn=function (data,index)
return (coef(lm(mpg~horsepower,data=data,subset=index))
set.seed(1)
boot.fn=function (data ,index)
return (coef(lm(mpg~horsepower,data=data,subset=index))
set.seed(1)
boot.fn(Auto,sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower,data=Auto))$coef
# Summary is correct
boot.fn=function(data,index)
coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))
set.seed(1)
boot(Auto,boot.fn,1000)
# Summary is correct
boot.fn=function(data,index){
coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))
}
set.seed(1)
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef
# Save history()
savehistory
# Save history()
savehistory()
q()
set.seed(1)
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef
boot.fn=function (data ,index)
return (coef(lm(mpg~horsepower,data=data,subset=index))
set.seed(1)
# It's all broken now
boot.fn(Auto,sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower,data=Auto))$coef
# Summary is correct
boot.fn=function(data,index)
coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))
set.seed(1)
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef
# Save history()
savehistory()
q()
# Clear workspace
rm(list=ls())
# Load the ISLR library
library(ISLR)
# Set seed for reproducible pseudorandom results
set.seed(1)
# Split observations into two halves
train=sample(392,196)
# Use subset option in lm() to fit a linear regression with observations corresponding to training data
lm.fit=lm(mpg~horsepower,data=Auto,subset=train)
# Predict response for 392 observations
# Calculate MSE of the 196 observations in validation set
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
# Use poly() to estimate test error for polynomial and cubic regressions
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(2)
train=sample(392,196)
lm.fit=lm(mpg~horsepower,subset=train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
lm.fit2=lm(mpg~poly(horsepower,2),data=Auto,subset=train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
lm.fit3=lm(mpg~poly(horsepower,3),data=Auto,subset=train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
# Using glm() function without passing the family argument, it performs linear regression
glm.fit=glm(mpg~horsepower,data=Auto)
coef(glm.fit)
lm.fit=lm(mpg~horsepower,data=Auto)
coef(lm.fit)
library(boot)
glm.fit=glm(mpg~horsepower,data=Auto)
cv.err=cv.glm(Auto,glm.fit)
cv.err$delta
# Repeat for complex polynomial fits
# Use for() to automate the program
cv.error=rep(0,5)
for (i in 1:5){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error[i]=cv.glm(Auto,glm.fit)$delta[1]
}
cv.error
set.seed(17)
cv.error.10=rep(0,10)
for (i in 1:10){
glm.fit=glm(mpg~poly(horsepower,i),data=Auto)
cv.error.10[i]=cv.glm(Auto,glm.fit,K=10)$delta[1]
}
cv.error.10
alpha.fn=function (data ,index){
X=data$X [index]
Y=data$Y [index]
return ((var(Y)-cov (X,Y))/(var(X)+var(Y) -2* cov(X,Y)))
}
alpha.fn(Portfolio,1:100)
set.seed(1)
alpha.fn(Portfolio,sample(100,100,replace=T))
boot(Portfolio,alpha.fn,R=1000)
boot.fn=function (data ,index)
return (coef(lm(mpg~horsepower,data=data,subset=index))
set.seed(1)
# It's all broken now
boot.fn(Auto,sample(392,392,replace=T))
boot.fn(Auto,sample(392,392,replace=T))
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower,data=Auto))$coef
# Summary is correct
boot.fn=function(data,index)
coefficients(lm(mpg~horsepower+I(horsepower^2),data=data,subset=index))
set.seed(1)
boot(Auto,boot.fn,1000)
summary(lm(mpg~horsepower+I(horsepower^2),data=Auto))$coef
# Save history()
savehistory()
q()
