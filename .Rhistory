# Each value of lambda is a vector of ridge regression coefficients
# Stored in a matrix accessible using coef()
dim(coef(ridge.mod))
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
# Coefficients when lambda = 705
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
# Use predict() to get ridge regression coefficients for new value of lambda, in this case 50
predict(ridge.mod,s=50,type="coefficients")[1:20,]
# Need to split samples into training/test sets
# Two ways to randomly split the data:
# 1) Produce random vector of TRUE and FALSE elements
# 2) Randomly choose a subset of numbers between 1 and n
# We use the second approach here
set.seed(1)
train=sample(1:nrow(x),nrow(x)/2)
test=(-train)
y.test=y[test]
# Fit ridge regression on training set and evaluate MSE on test set, using lambda = 4
# In this function call, newx replaces type="coefficients" to get predictions for test set
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=grid,thresh=1e-12)
ridge.pred=predict(ridge.mod,s=4,newx=x[test,])
mean((ridge.pred-y.test)^2)
# If we had fit a model with an intercept,
# we could predict each test observation using the mean of training observations
mean((mean(y[train])-y.test)^2)
# Could get the same result fitting a ridge regression model with very large lambda
ridge.pred=predict(ridge.mod,s=1e10,newx=x[test,])
mean((ridge.pred-y.test)^2)
# Check whether ridge regression with lambda = 4 is any better than least squares regression
# (least squares is just ridge regression with lambda = 0)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],exact=T)
# Check whether ridge regression with lambda = 4 is any better than least squares regression
# (least squares is just ridge regression with lambda = 0)
ridge.pred=predict(ridge.mod,s=0,newx=x[test,],x=x,y=y,exact=T)
mean((ridge.pred-y.test)^2)
# I got 94050, book got 114783
lm(y~x,subset=train)
predict(ridge.mod,s=0,exact=T,type="coefficients")[1:20,]
predict(ridge.mod,s=0,x=x,y=y,exact=T,type="coefficients")[1:20,]
# In general we should use cross-validation to choose the tuning parameter lambda
# Do this using cv.glmnet(), which performs ten-fold cross validation by default
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=0)
plot(cv.out)
bestlam=cv.out$lambda.min
bestlam
# Get MSE associated with this value
ridge.pred=predict(ridge.mod,s=bestlam,newx=x[test,])
mean((ridge.pred-y.test)^2)
# Fit ridge regression model on full dataset and examine coefficient estimates
out=glmnet(x,y,alpha=0)
predict(out,type="coefficients",s=bestlam)[1:20,]
# To fit lasso model, use glmnet() with alpha=1
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=grid)
plot(lasso.mod)
set.seed(1)
cv.out=cv.glmnet(x[train,],y[train],alpha=1)
plot(cv.out)
bestlam=cv.out$lambda.min
lasso.pred=predict(lasso.mod,s=bestlam,newx=x[test,])
mean((lasso.pred-y.test)^2)
# Advantage: resulting coefficients are sparse
out.glmnet(x,y,alpha=1,lambda=grid)
# Advantage: resulting coefficients are sparse
out=glmnet(x,y,alpha=1,lambda=grid)
lasso.coef=predict(out,type="coefficients",s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef!=0]
install.libraries('pls')
install.packages('pls')
# If not installed, install.packages('pls')
library(pls)
set.seed(2)
pcr.fit=pcr(Salary~.,data=Hitters,scale=TRUE,validation="CV")
# scale=TRUE standardises the predictors
# validation="CV" causes pcr() to compute ten-fold cross-validation error for each possible M (num components)
# Examine resulting fit
summary(pcr.fit)
# Plot cross-validation scores using validationplot()
# val.type="MSEP" causes cross-validation MSE to be plotted
validationplot(pcr.fit,val.type="MSEP")
# Perform PCR on training data and evaluate test set performance
set.seed(1)
pcr.fit=pcr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
validationplot(pcr.fit,val.type="MSEP")
pcr.pred=predict(pcr.fit,x[test,],ncomp=5)
mean((pcr.pred-y.test)^2)
pcr.pred=predict(pcr.fit,x[test,],ncomp=7)
mean((pcr.pred-y.test)^2)
# Fit PCR on full dataset, using M=5 (or M=7...)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=5)
summary(pcr.fit)
# Fit PCR on full dataset, using M=5 (or M=7...)
pcr.fit=pcr(y~x,scale=TRUE,ncomp=7)
summary(pcr.fit)
# Syntax for PLS is just like the syntax for PCR
set.seed(1)
pls.fit(plsr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV"))
pls.fit=plsr(Salary~.,data=Hitters,subset=train,scale=TRUE,validation="CV")
summary(pls.fit)
# My results differ
validationplot(pls.fit,val.type="MSEP")
# Also different, I get minimum at M=1, not M=2
pls.pred=predict(pls.fit,x[test,],ncomp=2)
mean((pls.pred-y.test)^2)
# Perform PLS using full data set and M=1 (or M=2??)
pls.fit=plsr(Salary~.,data=Hitters,scale=TRUE,ncomp=2)
summary(pls.fit)
# Perform PLS using full data set and M=1 (or M=2??)
pls.fit=plsr(Salary~.,data=Hitters,scale=TRUE,ncomp=1)
summary(pls.fit)
# Save history
savehistory()
# Quit
q()
# Clear workspace
rm(list=ls())
library(ISLR)
attach(Wage)
# Fit the model
fit=lm(wage~poly(age,4),data=Wage)
coef(summary(fit))
# We can use poly() to get age, age^2, age^3, age^4 directly with raw=TRUE
# This option affects the coefficients but not the fitted values
fit2=lm(wage~poly(age,4,raw=T),data=Wage)
coef(summary(fit2))
# Which is equivalent to this:
fit2a=lm(wage~age+I(age^2)+I(age^3)+I(age^4))
coef(fit2a)
# Or this:
fit2b=lm(wage~cbind(age,age^2,age^3,age^4),data=Wage)
coef(fit2b)
# Create grid of values for age
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
# Call predict() specifying that we want standard errors with se=TRUE
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
# Plot data and add the fit from deg-4 polynomial
par(mfrow=c(1,2),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# Fitted values are the same regardless of whether poly() produces orthogonal basis functions
preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE)
max(abs(pred$fit~preds2$fit))
max(abs(pred$fit-preds2$fit))
max(abs(preds$fit-preds2$fit))
# Fitted values are the same regardless of whether poly() produces orthogonal basis functions
preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2$fit))
# Create grid of values for age
agelims=range(age)
age.grid=seq(from=agelims[1],to=agelims[2])
# Call predict() specifying that we want standard errors with se=TRUE
preds=predict(fit,newdata=list(age=age.grid),se=TRUE)
se.bands=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
# Plot data and add the fit from deg-4 polynomial
# Divide plot area into 2 tall subplots. mar and oma allow us to control the margins
par(mfrow=c(1,2),mar=c(4.5,4.5,1,1),oma=c(0,0,4,0))
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
# Title spans both subplots
title("Degree-4 Polynomial",outer=T)
lines(age.grid,preds$fit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# Fitted values are the same regardless of whether poly() produces orthogonal basis functions
preds2=predict(fit2,newdata=list(age=age.grid),se=TRUE)
max(abs(preds$fit-preds2$fit))
# Must decide which degree of polynomial to use
# Use the analysis of variance anova() function to test H-null that M-1 sufficiently explains the data
# H-alt is that a more complex model M-2 is required
# M-1 and M-2 must be nested models (M-1's predictors must be a subset of M-2)
# Fit 5 models and sequentially compare the simpler model
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.2=lm(wage~poly(age,3),data=Wage)
fit.2=lm(wage~poly(age,4),data=Wage)
fit.2=lm(wage~poly(age,5),data=Wage)
# Must decide which degree of polynomial to use
# Use the analysis of variance anova() function to test H-null that M-1 sufficiently explains the data
# H-alt is that a more complex model M-2 is required
# M-1 and M-2 must be nested models (M-1's predictors must be a subset of M-2)
# Fit 5 models and sequentially compare the simpler model
fit.1=lm(wage~age,data=Wage)
fit.2=lm(wage~poly(age,2),data=Wage)
fit.3=lm(wage~poly(age,3),data=Wage)
fit.4=lm(wage~poly(age,4),data=Wage)
fit.5=lm(wage~poly(age,5),data=Wage)
anova(fit.1,fit.2,fit.3,fit.4,fit.5)
# Could have also used the orthogonal polynomials resulting from poly() to get p-values
coef(summary(fit.5))
# The square of the t-statistics are equal to the F-statistics from anova()
(-11.983)^2
olynomials
# Also works with other terms in the model
# We can compare these three models with anova()
# ANOVA works whether or not we use orthogonal polynomials
# ANOVA works whether or not we use orthogonal polynomials
# Also works with other terms in the model
# ANOVA works whether or not we use orthogonal polynomials
# Also works with other terms in the model
# We can compare these three models with anova()
# ANOVA works whether or not we use orthogonal polynomials
# Also works with other terms in the model
# We can compare these three models with anova()
fit.1=lm(wage~education+age,data=Wage)
fit.2=lm(wage~education+poly(age,2),data=Wage)
fit.3=lm(wage~education+poly(age,3),data=Wage)
anova(fit.1,fit.2,fit.3)
# Let's predict whether someone earns more that $250k per year
# Similar to before, except we first make an appropriate response vector
# And apply glm() using family="binomial" to fit a polynomial logistic regression
fit=glm(I(wage>250)~poly(age,4),data=Wage,family=binomial)
# wage>250 evaluates to a boolean TRUE or FALSE, while glm() turns it into binary (T=1, F=0)
# Make predictions
preds=predict(fit,newdata=list(age=age.grid),se=T)
# See the book for the very mathy equation for this command:
pfit=exp(preds$fit)/(1+exp(preds$fit))
se.bands.logit=cbind(preds$fit+2*preds$se.fit,preds$fit-2*preds$se.fit)
se.bands=exp(se.bands.logit)/(1+exp(se.bands.logit))
# Note: could have directly computed probabilities by selecting type="response" in predict()
preds=predict(fit,newdata=list(age=age.grid),type="response",se=T)
plot(age,I(wage>250),xlim=agelims,type="n",ylim=c(0,.2))
points(jitter(age),I((wage>250)/5),cex=.5,pch="|",col="darkgrey")
lines(age.grid,pfit,lwd=2,col="blue")
matlines(age.grid,se.bands,lwd=1,col="blue",lty=3)
# Fit a step function with cut()
table(cut(age,4))
fit=lm(wage~cut(age,4),data=Wage)
coef(summary(fit))
install.packages('splines')
install.packages('splines')
library(splines)
# Fit wage to age:
fit=lm(wage~bs(age,knots=c(25,40,60),data=Wage))
# Fit wage to age:
fit=lm(wage~bs(age,knots=c(25,40,60)),data=Wage)
pred=predict(fit,newdata=list(age=age.grid),se=T)
plot(age,wage,col="gray")
lines(age.grid,pred$fit,lwd=2)
lines(age.grid,pred$fit+2*pred$se,lty="dashed")
lines(age.grit,pred$fit-2*pred$se,lty="dashed")
lines(age.grid,pred$fit-2*pred$se,lty="dashed")
# Use df option to produce a spline with knots at uniform quantiles
dim(bs(age,knots=c(25,40,60)))
dim(bs(age,df=6))
attr(bs(age,df=6),"knots")
# Use ns() to fit a natural spline
fit2=lm(wage~ns(age,df=4),data=Wage)
pred2=predict(fit2,newdata=list(age=age.grid),se=T)
lines(age.grid,pred2$fit,col="red",lwd=2)
# Fit a smoothing spline
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Smoothing Spline")
fit=smooth.spline(age,wage,df=16)
fit2=smooth.spline(age,wage,cv=TRUE)
fit2$df
lines(fit,col="red",lwd=2)
lines(fit2,col="blue",lwd=2)
legend("topright",legend=c("16 DF","6.8 DF"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
# Perform local regression with loess() function
plot(age,wage,xlim=agelims,cex=.5,col="darkgrey")
title("Local Regression")
fit=loess(wage~age,span=.2,data=Wage)
fit2=loess(wage~age,span=.5,data=Wage)
lines(age.grid,predct(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit,data.frame(age=age.grid)),col="red",lwd=2)
lines(age.grid,predict(fit2,data.frame(age=age.grid)),col="blue",lwd=2)
legend("topright",legend=c("Span=0.2","Span=0.5"),col=c("red","blue"),lty=1,lwd=2,cex=.8)
education as a qualitative predictor
# gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage
# Fit GAM to predict wage using year and age, treating education as a qualitative predictor
gam1=lm(wage~ns(year,4)+ns(age,5)+education,data=Wage)
install.packages('gam')
# Fit model using smoothing splines instead of natural splines
# Need gam library. If not installed, install.packages('gam')
library(gam)
# s() indicates we want a smoothing spline
# We specify 4 deg of freedom for year and 5 for age
# Qualitative variables can be left as they are, as it will be converted into dummy variables
# Use gam() to fit the function
gam.m3=gam(wage~s(year,4)+s(age,5)+education,data=Wage)
# Plot the figure
par(mfrow=c(1,3))
plot(gam.m3,se=TRUE,col="blue")
# We can still use plot.gam() on gam1 even though it is an lm
plot.gam(gam1,se=TRUE,col="red")
# We can still use plot.gam() on gam1 even though it is an lm
plot.gam(gam1,se=TRUE,col="red")
# We can still use plot.gam() on gam1 even though it is an lm
plot.gam(gam1,se=TRUE,col="red")
# We can still use plot.gam() on gam1 even though it is an lm
plot.Gam(gam1,se=TRUE,col="red")
# We can use ANOVA tests to determine which model is best
# M-1: a GAM that excludes year
# M-2: a GAM that uses a linear function of year
# M-3: a GAM that uses a spline function of year
gam.m1=gam(wage~s(age,5)+education,data=Wage)
gam.m2=gam(wage~year+s(age,5)+education,data=Wage)
anova(gam.m1,gam.m2,gam.m3,test="F")
summary(gam.m3)
# Make predictions
preds=predict(gam.m2,newdata=Wage)
# We can use local regression as building blocks in a GAM with lo()
gam.lo=gam(wage~s(year,df=4)+lo(age,span=0.7)+education,data=Wage)
plot.Gam(gam.lo,se=TRUE,col="green")
# We can also use lo() to create interactions before calling the gam() function
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
# We can also use lo() to create interactions before calling the gam() function
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
# We can also use lo() to create interactions before calling the gam() function
gam.lo.i=gam(wage~lo(year,age,span=0.5)+education,data=Wage)
# To plot resulting 2D surface, install akima
install.packages('akima')
library(akima)
plot(gam.lo.i)
# To fit logistic regression GAM, use I() to make binary response variable with family=binomial
gam.lr=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage)
par(mfrow=c(1,3))
plot(gam.lr,se=T,col="green")
# There are no high earners in the <HS category
table(education,I(wage>250))
# So we can remove this category from the GAM
gam.lr.s=gam(I(wage>250)âˆ¼year+s(age,df=5)+education,family=binomial,data=Wage,subset=(education!="1. < HS Grad"))
# So we can remove this category from the GAM
gam.lr.s=gam(I(wage>250)~year+s(age,df=5)+education,family=binomial,data=Wage,subset=(education!="1. < HS Grad"))
plot(gam.lr.s,se=T,col="green")
# Save history
savehistory()
# Quit
quit()
# Quit
q()
# Clear workspace
rm(list=ls())
install.packages('tree')
# If not already installed, install.packages('tree')
library(tree)
# Task: analyse Carseats data
# Recode Sales as binary variable
library(ISLR)
attach(Carseats)
High=ifelse(Sales<=8,"no","yes")
# Creat data frame to merge High with Carseats
Carseats=data.frame(Carseats,High)
# Fit classification tree to predict High using all variables except Sales
tree.carseats=tree(High~.-Sales,Carseats)
# Summarise tree
summary(tree.carseats)
# Plot the tree, with pretty=0 to include category names
plot(tree.carseats)
text(tree.carseats,pretty=0)
tree.carseats
# Split into test and train sets and predict results
set.seed(2)
train=sample(1:nrow(Carseats),200)
Carseats.test=Carseats[-train,]
High.test=High[-train,]
High.test=High[-train]
tree.carseats=tree(High~.-Sales,Carseats,subset=train)
tree.pred=predict(tree.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(104+50)/200
# Use pruning to attempt to improve model
# Determine optimal tree complexity with cv.tree()
set.seed(3)
# Determine optimal tree complexity with cv.tree()
# FUN=prune.misclass indicates we want classification error rate to guide cv and pruning
# As opposed to the cv.tree() default which is deviance
cv.carseats=cv.tree(tree.carseats,FUN=prune.misclass)
names(cv.carseats)
cv.carseats
# Different values from book
attr(,"class")
# Different values from book
attr("class")
# Different values from book
attr(,"class")
# Different values from book
attr( ,"class")
# Plot error rate as function of size and k
par(mfrow=c(1,2))
plot(cv.carseats$size,cv.carseats$dev,type="b")
plot(cv.carseats$k,cv.carseats$dev,type="b")
# Apply prune.misclass() to prune tree down to 8 nodes (book says 9 but my results differ)
prune.carseats=prune.misclass(tree.carseats,best=8)
plot(prune.carseats)
text(prune.carseats,pretty=0)
# Determine accuracy
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
(89+62)/200
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.caarseats,best=15)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=15)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=10)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=11)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
# Increase the value of best for larger pruned tree with lower accuracy
prune.carseats=prune.misclass(tree.carseats,best=12)
plot(prune.carseats)
text(prune.carseats,pretty=0)
tree.pred=predict(prune.carseats,Carseats.test,type="class")
table(tree.pred,High.test)
library(MASS)
set.seed(1)
train=sample(1:nrow(Boston),nrow(Boston)/2)
tree.boston=tree(medv~.,Boston,subset=train)
summary(tree.boston)
plot(tree.boston)
text(tree.boston,pretty=0)
# Determine if pruning helps
cv.boston=cv.tree(tree.boston)
plot(cv.boston$size,cv.boston$dev,type='b')
prune.boston=prune.tree(tree.boston,best=5)
plot(prune.boston)
test(prune.boston,pretty=0)
text(prune.boston,pretty=0)
# Use unpruned tree to make predictions
yhat=predict(tree.boston,newdata=Boston[-train,])
boston.test=Boston[-train,"medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
# Apply bagging and random forests to Boston data
install.packages('randomForest')
# Apply bagging and random forests to Boston data
# If not already installed, install.packages('randomForest')
library(randomForest)
set.seed(1)
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,importance=TRUE)
bag.boston
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
plot(yhat.bag)
abline(0,1)
# That doesn't look right...
mean((yhat.bag-boston.test)^2)
# Change number of trees grown by randomForest() using ntree arg
bag.boston=randomForest(medv~.,data=Boston.subset=train,mtry=13,tree=25)
# Change number of trees grown by randomForest() using ntree arg
bag.boston=randomForest(medv~.,data=Boston,subset=train,mtry=13,tree=25)
yhat.bag=predict(bag.boston,newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
# Try with mtry=6
set.seed(1)
rf.boston=randomForest(medv~.,data=Boston,subset=train,mtry=6,importance=TRUE)
yhat.rf=predict(rf.boston,newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
# View importance of variables
importance(rf.boston)
varImpPlot(rf.boston)
install.packages('gbm')
# If not already installed, install.packages('gbm')
library(gbm)
# Run gbm() with distribution="gaussian" because this is regression
# If for classification, use distribution="bernoulli"
# Specify number of trees with n.trees=5000
# Limit depth with interaction.depth=4
set.seed(1)
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4)
summary(boost.boston)
# Produce partial dependence plots, showing marginal effect of selected variables on response
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Produce partial dependence plots, showing marginal effect of selected variables on response
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Produce partial dependence plots, showing marginal effect of selected variables on response
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
# Use boosted model to predict medv on test set
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat.boost-boston.test)^2)
# Perform boosting with different shrinkage parameter (lambda). Default = 0.001
boost.boston=gbm(medv~.,data=Boston[train,],distribution="gaussian",n.trees=5000,interaction.depth=4,shrinkage=0.2,verbose=F)
yhat.boost=predict(boost.boston,newdata=Boston[-train,],n.trees=5000)
mean((yhat,boost-boston.test)^2)
mean((yhat.boost-boston.test)^2)
# Save history
savehistory()
# Quit
quit()
# Quit
quit()
